# ğŸ“Š Sentiment Analysis - Deep Learning Models

> A comprehensive deep learning project implementing three neural network architectures for sentiment classification of product reviews. Achieves 91.2% accuracy with Attention-BiLSTM model.

[![GitHub](https://img.shields.io/badge/GitHub-ee6483project-blue?logo=github)](https://github.com/TSWtswTSWtsw123/ee6483project)
[![Python](https://img.shields.io/badge/Python-3.8+-blue?logo=python)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-Latest-red?logo=pytorch)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-Educational-green)](LICENSE)

## ğŸ¯ Overview

This project implements comprehensive sentiment classification models:

### Deep Learning Models (Neural Networks)
- **CNN Classifier**: Multiple parallel convolutions with different filter sizes to capture n-gram features (86.3% accuracy)
- **BiLSTM Classifier**: Bidirectional LSTM processing sequences in both directions (88.7% accuracy)
- **Attention-BiLSTM Classifier** â­: Enhanced BiLSTM with attention mechanism for interpretability **(91.2% accuracy)**

### Traditional Machine Learning Models (Baselines)
- **Support Vector Machine (SVM)**: Linear SVM with TF-IDF features
- **Logistic Regression**: Efficient baseline with L2 regularization
- **Naive Bayes**: Probabilistic classifier for quick comparison

**Course**: IE6483 / EE6483 Mini Project - Artificial Intelligence and Data Mining
**Institution**: Nanyang Technological University (NTU)

## ğŸ“ Project Structure

```
final/
â”œâ”€â”€ Source Code/                      # Python implementation files
â”‚   â”œâ”€â”€ deep_learning_models.py      # Deep learning model implementations
â”‚   â”œâ”€â”€ data_utils.py                # Data loading and preprocessing
â”‚   â”œâ”€â”€ train.py                     # Deep learning training script
â”‚   â”œâ”€â”€ predict.py                   # Prediction script
â”‚   â”œâ”€â”€ run_all.py                   # Complete pipeline (DL models)
â”‚   â”œâ”€â”€ example_usage.py             # Usage examples
â”‚   â””â”€â”€ SVM_LR_NB.py                 # Traditional ML baselines
â”‚
â”œâ”€â”€ Configuration & Results/          # Dependencies and results
â”‚   â”œâ”€â”€ requirements.txt              # Python dependencies
â”‚   â”œâ”€â”€ training_results.json         # Validation metrics
â”‚   â””â”€â”€ submission.csv                # Test predictions
â”‚
â”œâ”€â”€ Data Files/                       # Training and test datasets
â”‚   â”œâ”€â”€ train.json                   # 7,401 training samples
â”‚   â””â”€â”€ test.json                    # 1,851 test samples
â”‚
â”œâ”€â”€ Models/                           # Pre-trained model weights
â”‚   â”œâ”€â”€ best_cnn_model.pt
â”‚   â”œâ”€â”€ best_bilstm_model.pt
â”‚   â””â”€â”€ best_attention_bilstm_model.pt
â”‚
â”œâ”€â”€ Visualizations/                   # Performance analysis charts
â”‚   â”œâ”€â”€ accuracy_vs_time.png
â”‚   â”œâ”€â”€ all_models_comparison.png
â”‚   â”œâ”€â”€ attention_visualization.png
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ roc_curve.png
â”‚   â”œâ”€â”€ training_accuracy.png
â”‚   â””â”€â”€ training_loss.png
â”‚
â”œâ”€â”€ Documentation/                    # Comprehensive guides
â”‚   â”œâ”€â”€ README.md                    # Full project guide
â”‚   â”œâ”€â”€ QUICKSTART.md                # Quick start instructions
â”‚   â”œâ”€â”€ PROJECT_SUMMARY.md           # Project overview
â”‚   â”œâ”€â”€ FINAL_REPORT.md              # Detailed final report
â”‚   â””â”€â”€ GITHUB_UPLOAD_INSTRUCTIONS.md
â”‚
â”œâ”€â”€ Research & References/            # Academic papers and sources
â”‚   â”œâ”€â”€ merged_report.pdf
â”‚   â”œâ”€â”€ deeplearningpart.pdf
â”‚   â”œâ”€â”€ IE6483-Project1.pdf
â”‚   â””â”€â”€ [More references...]
â”‚
â”œâ”€â”€ Logs/                             # Training logs
â”‚   â”œâ”€â”€ training.log
â”‚   â””â”€â”€ full_training.log
â”‚
â””â”€â”€ Notebooks/                        # Jupyter notebooks
    â”œâ”€â”€ Sentiment_Analysis_Models.ipynb
    â”œâ”€â”€ tfidf_features_example.ipynb
    â””â”€â”€ [More notebooks...]
```

## ğŸš€ Quick Start

### 1. Prerequisites

- Python 3.8 or higher
- pip package manager
- Git

### 2. Installation

```bash
cd Source\ Code
pip install -r ../Configuration\ \&\ Results/requirements.txt
```

### 3. Run the Project

**Option A: Complete Pipeline (Train + Predict)**
```bash
cd Source\ Code
python run_all.py
```

**Option B: Train Only**
```bash
cd Source\ Code
python train.py
```

**Option C: Generate Predictions**
```bash
cd Source\ Code
python predict.py
```

**Option D: Run Examples**
```bash
cd Source\ Code
python example_usage.py
```

## ğŸ“Š Model Performance

| Model | Accuracy | Precision | Recall | F1-Score | Training Time |
|-------|----------|-----------|--------|----------|---------------|
| CNN | 86.3% | 0.862 | 0.863 | 0.863 | ~45 min |
| BiLSTM | 88.7% | 0.884 | 0.887 | 0.886 | ~90 min |
| **Attention-BiLSTM** | **91.2%** | **0.908** | **0.912** | **0.910** | **~100 min** |

### Class-wise Performance (Best Model)
- Positive class recall: 92.0%
- Negative class recall: 91.6%
- Balanced performance across both classes

## ğŸ—ï¸ Architecture Details

### Attention-BiLSTM (Primary Model)
```
Input â†’ Embedding (300-dim)
       â†’ BiLSTM (2 layers, 256 units each)
       â†’ Attention Mechanism
       â†’ Fully Connected (512 â†’ 128 â†’ 1)
       â†’ Sigmoid (Binary Classification)
```

**Key Features:**
- Attention weights reveal which words influence the prediction
- 300-dimensional word embeddings
- 2 stacked bidirectional LSTM layers
- Attention-based context vector
- Dropout (0.5) for regularization
- Batch normalization for stability

## ğŸ“š Documentation

For detailed information, see:

1. **[Documentation/README.md](Documentation/README.md)** - Complete project guide with full details
2. **[Documentation/QUICKSTART.md](Documentation/QUICKSTART.md)** - Step-by-step quick start
3. **[Documentation/PROJECT_SUMMARY.md](Documentation/PROJECT_SUMMARY.md)** - High-level overview
4. **[Documentation/FINAL_REPORT.md](Documentation/FINAL_REPORT.md)** - Detailed final report
5. **[UPLOAD_SUCCESS.md](UPLOAD_SUCCESS.md)** - GitHub upload confirmation

## ğŸ’¾ Data

**Training Data**: 7,401 product reviews with sentiment labels
**Test Data**: 1,851 product reviews for evaluation
**Class Distribution**: ~85% positive, ~15% negative (6:1 imbalance)
**Sequence Length**: Fixed at 200 tokens (covers 91.3% of reviews)

## ğŸ”§ Key Features

### Data Preprocessing
- Text normalization (lowercase, remove URLs/emails)
- NLTK tokenization
- Vocabulary building (15,247 words)
- Sequence padding/truncation
- Special tokens: `<PAD>` (index 0), `<UNK>` (index 1)

### Training Features
- **Loss Function**: Weighted Binary Cross-Entropy (handles class imbalance)
- **Optimizer**: Adam with learning rate 0.001
- **Batch Size**: 64
- **Early Stopping**: Patience = 5 epochs
- **Learning Rate Scheduling**: ReduceLROnPlateau (reduce by 0.5 every 2 epochs)
- **Gradient Clipping**: max_norm = 1.0
- **Reproducibility**: Fixed random seed = 42

### Regularization
- Dropout (0.5) for overfitting prevention
- Batch normalization for training stability
- Weighted loss for class imbalance handling
- Gradient clipping for stability

## ğŸ“ Model Architecture Comparison

### CNN
- Parallel Conv1D layers (filters: 3, 4, 5)
- Global max pooling
- Fast training (~45 min)
- Good for local feature extraction

### BiLSTM
- Bidirectional LSTM (2 stacked layers)
- Captures long-range dependencies
- Medium training time (~90 min)
- Better context understanding

### Attention-BiLSTM â­
- BiLSTM + Attention mechanism
- Interpretable predictions
- Longest training time (~100 min)
- Best performance (91.2%)
- Attention weights show influential words

## ğŸ” Model Interpretability

The Attention-BiLSTM model provides interpretability through:
- **Attention Weights**: Shows which words influenced the prediction
- **Error Analysis**: Understanding failure cases
- **Example Cases**: Detailed examples of correct and incorrect predictions

## âš™ï¸ Hardware Requirements

- **CPU**: Intel i5/i7 or AMD Ryzen 5/7+ (minimum)
- **GPU**: NVIDIA GTX 1080+ recommended (tested on RTX 2080 Ti)
- **RAM**: Minimum 8GB, recommended 16GB+
- **Storage**: ~4GB for model weights + data files

## ğŸ› ï¸ Troubleshooting

### "No such file or directory" error
**Solution**: Ensure you're running scripts from the correct directory

### Out of Memory (OOM) error
**Solution**: Reduce batch size in train.py (modify `batch_size=64`)

### Model training is very slow
**Solution**: Check CUDA/GPU support with `torch.cuda.is_available()`

## ğŸ“ˆ Project Highlights

âœ… **Complete Implementation** - 3 different neural network architectures
âœ… **High Performance** - 91.2% accuracy on validation set
âœ… **Well Documented** - Comprehensive README and documentation
âœ… **Organized Structure** - 9 logical project folders
âœ… **Production Ready** - Clean, well-commented source code
âœ… **Reproducible** - Fixed seeds and detailed training logs
âœ… **Visualized Results** - 7 performance analysis charts
âœ… **GitHub Ready** - Professional repository structure

## ğŸ“„ Files Overview

### Source Code
- `deep_learning_models.py` (420 lines) - All three deep learning model implementations (CNN, BiLSTM, Attention-BiLSTM)
- `data_utils.py` (310 lines) - Data loading and preprocessing utilities
- `train.py` (380 lines) - Complete training pipeline for deep learning models
- `predict.py` (220 lines) - Prediction generation script
- `run_all.py` (95 lines) - Orchestrates train + predict for DL models
- `example_usage.py` (180 lines) - Usage examples and demonstrations
- `SVM_LR_NB.py` (251 lines) - Traditional ML baseline models (SVM, Logistic Regression, Naive Bayes)

### Configuration
- `requirements.txt` - All Python dependencies
- `training_results.json` - Validation metrics for all models

### Results
- `submission.csv` - Test predictions (2,851 samples)

## ğŸ¯ Project Completion Checklist

- âœ… Literature survey on sentiment analysis
- âœ… Feature format selection (learned word embeddings)
- âœ… Multiple model architectures (CNN, BiLSTM, Attention-BiLSTM)
- âœ… Hyperparameter optimization and ablation study
- âœ… Error analysis with example cases
- âœ… Feature format impact analysis
- âœ… Domain adaptation strategy (hotel reviews)
- âœ… Noisy label handling approaches
- âœ… Project structure organized into 9 folders
- âœ… Comprehensive documentation
- âœ… GitHub repository uploaded

## ğŸ”— Repository Information

- **GitHub Repository**: https://github.com/TSWtswTSWtsw123/ee6483project
- **Owner**: TSWtswTSWtsw123
- **Email**: 1072202885@qq.com
- **Branch**: main
- **Total Commits**: 8+
- **Last Updated**: 2025-11-14

## ğŸ“– References

- Devlin et al. (2019): BERT - Pre-training of Deep Bidirectional Transformers
- Pennington et al. (2014): GloVe - Global Vectors for Word Representation
- Mikolov et al. (2013): Efficient Estimation of Word Representations in Vector Space
- Kim (2014): Convolutional Neural Networks for Sentence Classification
- Bahdanau et al. (2015): Neural Machine Translation by Jointly Learning to Align and Translate

## ğŸ“ License

This project is provided as-is for educational purposes.

## ğŸ¤ Contributing

This is a course project. For modifications or improvements, please fork the repository and create a pull request.

## â“ FAQ

**Q: How do I clone this project?**
```bash
git clone https://github.com/TSWtswTSWtsw123/ee6483project.git
cd ee6483project
```

**Q: Which model should I use?**
A: The Attention-BiLSTM model offers the best balance of accuracy (91.2%) and interpretability.

**Q: How long does training take?**
A: Approximately 100 minutes on an RTX 2080 Ti GPU. CPU training will take significantly longer.

**Q: Can I use pre-trained models?**
A: Yes, all three models are pre-trained and saved in the `Models/` folder.

**Q: How do I understand the model decisions?**
A: The Attention-BiLSTM model provides attention weights showing which words influenced each prediction.

## ğŸ“ Support

For questions, issues, or suggestions:
1. Check the [Documentation](Documentation/) folder
2. Review the [FINAL_REPORT.md](Documentation/FINAL_REPORT.md)
3. Check existing issues on GitHub
4. Open a new issue with detailed description

---

**Project Status**: âœ… Complete and uploaded to GitHub
**Last Updated**: 2025-11-14
**Course**: IE6483 / EE6483 - Artificial Intelligence and Data Mining

**Made with â¤ï¸ for EE6483 Mini Project**
